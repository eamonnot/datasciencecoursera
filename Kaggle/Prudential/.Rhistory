cy3 <- cheby1(4,0.1,0.04,type="low")
filteredDiff <- filtfilt(cy3, diffExFilt)
plot.ts(filteredDiff)
cy3 <- cheby1(4,1,0.04,type="low")
filteredDiff <- filtfilt(cy3, diffExFilt)
plot.ts(filteredDiff)
cy3 <- cheby1(4,5,0.04,type="low")
filteredDiff <- filtfilt(cy3, diffExFilt)
plot.ts(filteredDiff)
cy3 <- cheby1(4,5,0.01,type="low")
filteredDiff <- filtfilt(cy3, diffExFilt)
cy3 <- cheby1(4,5,0.01,type="low")
changeP(congest$Congestion)
abline(v=307, col="green")
abline(v=391, col="green")
abline(v=401, col="green")
abline(v=488, col="green")
abline(v=780, col="green")
abline(v=859, col="green")
abline(v=536, col="red")
abline(v=624, col="red")
abline(v=1072, col="red")
abline(v=1158, col="red")
bf <- butter(4, 1/40, type="low")
filteredX <- filtfilt(bf,congest$Congestion)
plot.ts(filteredX)
plot.ts(diff(filteredX))
congest2 <- read.table("~/congest2.csv", header=TRUE, quote="\"")
View(congest2)
filteredX2 <- filtfilt(bf,congest2$Congestion)
plot.ts(filteredX2)
plot.ts(congest2$Congestion)
plot.ts(filteredX2)
install.packages("KernSmooth")
library(KernSmooth)
library(eamonnFunc)
RunSummRun52 <- read.csv("~/netlogo/EmergenceTests/Traffic2/Summary/RunSummRun52.csv")
View(RunSummRun52)
changeP(RunSummRun52$Object)
plot.ts(RunSummRun52$Object)
abline(v=262, col="red")
abline(v=515, col="red")
abline(v=617, col="red")
abline(v=690, col="red")
abline(v=797, col="red")
abline(v=902, col="red")
abline(v=953, col="red")
abline(v=1003, col="red")
abline(v=519, col="blue")
abline(v=609, col="blue")
abline(v=737, col="blue")
abline(v=796, col="blue")
abline(v=905, col="blue")
abline(v=952, col="blue")
abline(v=1004, col="blue")
abline(v=1068, col="blue")
lines(RunSummRun52$Change, col="green")
lines(RunSummRun52$Emerge, col="yellow")
library(devtools)
ageLasso <- read.csv("~/ageLasso.csv")
View(ageLasso)
XGroup<-cbind(ageLasso$BirdsNear,ageLasso$DistNear,ageLasso$FlockHead,ageLasso$FlockSpeed,ageLasso$Temperature)
lassoRun.cv<-cv.glmnet(XGroup,ageLasso$Age)
library(glmnet)
lassoRun.cv<-cv.glmnet(XGroup,ageLasso$Age)
lassofit<-glmnet(XGroup,y,alpha=1,nlambda=100)
lassofit<-glmnet(XGroup,ageLasso$Age,alpha=1,nlambda=100)
lassopred<-predict(lassofit,XGroup,s=lassoRun.cv$lambda.min)
lassocoef<-predict(lassofit,s=lassoRun.cv$lambda.min,type="coefficients")
lassocoef[2,1]
lassocoef[3,1]
lassocoef[4,1]
lassocoef[5,1]
lassocoef
with(ageLasso,plot(Age~BirdsNear))
with(ageLasso,plot(Age~DistNear))
with(ageLasso,plot(Age~FlockHead))
with(ageLasso,plot(Age~FlockSpeed))
with(ageLasso,plot(Age~Temperature))
lassocoef[6,1]
plot(lassoRun.cv)
plot(lassofit)
lassopred
lassofit
summary(lassofit)
plot(lassofit,xvar="lambda")
plot(lassoRun.cv)
lassoRun.cv$lambda.min
install.packages("lars")
require(lars)
data(diabetes)
cv.res <- cv.lars(diabetes$x,diabetes$y,type="lasso", mode="fraction", plot=FALSE)
opt.frac <- min(cv.res$cv) + sd(cv.res$cv)
opt.frac <- cv.res$index[which(cv.res$cv > opt.frac)[1]]
lasso.path <- lars(diabetes$x,diabetes$y, type="lasso")
lasso.fit <- predict.lars(lasso.path, type="coefficients", mode="fraction", s = opt.frac)
coef(lasso.fit)
opt.frac <- cv.res$index[which(cv.res$cv < opt.frac)[1]]
lasso.path <- lars(diabetes$x,diabetes$y, type="lasso")
lasso.fit <- predict.lars(lasso.path, type="coefficients", mode="fraction", s = opt.frac)
coef(lasso.fit)
plot(lassofit, xvar="lambda", label = TRUE)
lassoRun.cv$lambda.min
log(lassoRun.cv$lambda.min)
plot(lassofit, xvar="dev", label = TRUE)
coef(lassoRun.cv, s="lambda.min")
fit.lasso.cv2 <- cv.glmnet(XGroup,ageLasso$Age,alpha=1,nlambda=100, type.measure = "mse")
coef(fit.lasso.cv2, s="lambda.min")
lm(ageLasso$Age, ageLasso$BirdsNear)
lm(ageLasso$Age ~ ageLasso$BirdsNear)
summary(lm(ageLasso$Age ~ ageLasso$BirdsNear))
summary(lm(ageLasso$Age ~ ageLasso$DistNear))
summary(lm(ageLasso$Age ~ ageLasso$FlockHead))
plot(lm(ageLasso$Age ~ ageLasso$FlockHead))
summary(lm(ageLasso$Age ~ ageLasso$FlockSpeed))
summary(lm(ageLasso$Age ~ ageLasso$Temperature))
summary(lm(ageLasso$Age ~ ageLasso$BirdsNear+ ageLasso$DistNear + ageLasso$FlockHead + ageLasso$FlockSpeed + ageLasso$Temperature))
plot(lm(ageLasso$Age ~ ageLasso$BirdsNear+ ageLasso$DistNear + ageLasso$FlockHead + ageLasso$FlockSpeed + ageLasso$Temperature))
heightLasso <- read.csv("~/heightLasso.csv")
View(heightLasso)
XGroup<-with(heightLasso,cbind(BirdsNear,DistNear,FlockHead,FlockSpeed,Temperature))
lassoRun.cv<-cv.glmnet(XGroup,heightLasso$Height)
lassofit<-glmnet(XGroup,heightLasso$Height,alpha=1,nlambda=100)
lassopred<-predict(lassofit,XGroup,s=lassoRun.cv$lambda.min)
lassocoef<-predict(lassofit,s=lassoRun.cv$lambda.min,type="coefficients")
lassocoef[2,1]
lassocoef[3,1]
lassocoef[4,1]
lassocoef[5,1]
lassocoef[6,1]
lassocoef[6,1]
library(eamoFunc)
library(eamonnFunc)
turtle219 <- read.csv("~/netlogo/LassoTest/VarFiles/turtle219.csv")
View(turtle219)
injectNoise <- function(x){
rows <- nrow(x)
cols <- ncol(x)
set.seed(332)
for(i in 1:rows){
for(j in 1:cols){
theNoise = runif(n = 1, min = -0.05, max = 0.05)
x[i,j] <- x[i,j] + theNoise
}
}
return(x)
}
noiseTurt <- injectNoise(turtle219)
library(eamonnFunc)
eamonnFunc
library(eamonnFunc)
chooseVariables(noiseTurt[,1:5],noiseTurt[,6:10])
version
setwd("~/Coursera//DataScienceSpecial//OwnRepos//PracticalMachineLearning")
list.files()
setwd("ProjectRepo/")
list.files()
# Step 1: Load Libraries
library(caret)
library(doParallel)
registerDoParallel(cores=2)
# Step 2 - Get and load the data
checkAndDownloadFiles <- function(){
## First check if data folder exists
if (!dir.exists("data")){
dir.create(path = "data")
}
if (!file.exists("data/pml-training.csv")){
fileURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url = fileURL, destfile = "data/pml-training.csv", method="curl")
}
if (!file.exists("data/pml-testing.csv")){
fileURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url = fileURL, destfile = "data/pml-testing.csv", method="curl")
}
}
readInTrainingData <- function(){
filename <- "data/pml-training.csv"
training <- read.table(filename, sep=",", skip=0, header=TRUE,
na.strings=c("NA","NaN",""," ","#DIV/0!"))
return (training)
}
checkAndDownloadFiles()
allTrainingData <- readInTrainingData()
# Step 3 - Partition training & test datasets
createTrainTest <- function(theData){
set.seed(41282)
inTrain <- createDataPartition(y = theData$classe, p=.70, list=FALSE)
training <- theData[inTrain,]
testing <- theData[-inTrain,]
return (list(training = training, testing = testing))
}
datasets <- createTrainTest(allTrainingData)
training <- datasets$training
# Step 4 - Process the data
# A. Remove columns that are mostly NA
training <- training[,colSums(is.na(training)) < nrow(training) * 0.5]
any(is.na(training))
# B. Next Remove columsn relating to Index, timestamps and windows
training <- training[,-(1:7)]
# C. Check for highly correlated features and remove unnecessary features
descrCor <- cor(training[,-ncol(training)])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
training <- training[,-highlyCorDescr]
# Step 5 - Training
lastCol <- ncol(training)
preProc.Norm <- preProcess(training[,-lastCol],method=c("center","scale"))
train.Norm <- predict(preProc.Norm, training[,-lastCol])
set.seed(360)
if(!file.exists("model_Norm.Rds")){
print("Model file for Center+Scale not found. About to train new model.
Go get a coffee, this will take a while")
modelFit.Norm <- train(training$classe ~., method="rf" ,data=train.Norm,
trControl=trainControl(method="cv",number=5),
prox=TRUE,allowParallel=TRUE)
saveRDS(modelFit.Norm, "model_Norm.Rds")
registerDoParallel(cores=1)
}else{
print("Loading Ceneter+Scale model")
modelFit.Norm <- readRDS("model_Norm.Rds")
}
print("FinalModel")
modelFit.Norm$finalModel
modelFit.Norm$pred
modelFit.Norm$results
modelFit.Norm$bestTune
modelFit.Norm$maximize
modelFit.Norm$modelInfo
source('~/Coursera/DataScienceSpecial/OwnRepos/PracticalMachineLearning/ProjectRepo/trainAndTest.R')
print(modelFit.Norm, verbose = FALSE)
print(modelFit.Norm$finalModel, verbose = FALSE)
require(randomForest)
source('~/Coursera/DataScienceSpecial/OwnRepos/PracticalMachineLearning/ProjectRepo/trainAndTest.R')
setwd("~/")
library(eamonnFunc)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
View(vowel.test)
View(vowel.train)
library(caret)
source('~/.active-rstudio-document')
predRF <- predict(modelRF,vowel.test)
predGBM <- predict(modelGBM,vowel.test)
table(predRF,vowel.test$y)
class(vowel.test$y)
# Question 1
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
modelRF <- train(y ~., method = "rf", data=vowel.train)
modelGBM <- train(y ~., method = "gbm", data=vowel.train)
predRF <- predict(modelRF,vowel.test)
predGBM <- predict(modelGBM,vowel.test)
modelGBM <- train(y ~., method = "gbm", data=vowel.train, verbose=FALSE)
vowel.test$predRF <- predRF
vowel.test$predGBM <- predGBM
vowel.test$Agree <- if(vowel.test$predRF == vowel.test$predGBM){TRUE}else{FALSE}
View(vowel.test)
vowel.test$Agree <- if(vowel.test$predRF == vowel.test$predGBM){vowel.test$Agree = TRUE}else{vowel.test$Agree = FALSE}
vowel.test$Agree <- ifelse(vowel.test$predRF == vowel.test$predGBM, TRUE, FALSE}
vowel.test$Agree <- ifelse(vowel.test$predRF == vowel.test$predGBM, TRUE, FALSE)
View(vowel.test)
vowel.test$RFRight <- vowel.test$predRF == vowel.test$y
vowel.test$GBMRight <- vowel.test$predGBM == vowel.test$y
vowel.test$AgreeRight <- if(vowel.test$Agree, if(vowel.test$predRF == vowel.test$y,TRUE,FALSE),FALSE)
vowel.test$AgreeRight <- ifelse(vowel.test$Agree, ifelse(vowel.test$predRF == vowel.test$y,TRUE,FALSE),FALSE)
print(nrow(vowel.test[vowel.test$RFRight == TRUE,]) / nrow(vowel.test))
print(nrow(vowel.test[vowel.test$GBMRight == TRUE,]) / nrow(vowel.test))
print(nrow(vowel.test[vowel.test$RFRight == TRUE,]) / nrow(vowel.test))
print(nrow(vowel.test[vowel.test$GBMRight == TRUE,]) / nrow(vowel.test))
print(nrow(vowel.test[vowel.test$AgreeRight == TRUE,]) / nrow(vowel.test[vowel.test$Agree == TRUE,]))
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
class(training$diagnosis)
set.seed(62433)
modelRF <- train(diagnosis ~., method = "rf", data=training)
modelGBM <- train(diagnosis ~., method = "gbm", data=training, verbose=FALSE)
modelLDA <- train(diagnosis ~., method = "lda", data=training)
predRF <- predict(modelRF,testing)
predGBM <- predict(modelGBM,testing)
predLDA <- predict(modelLDA,testing)
predDF <- data.frame(predRF,predGBM,predLDA,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~., method="rf", data=predDF)
combPred <- predict(combModFit, predDF)
testing$RFRight <- predRF == testing$diagnosis
testing$RFRight <- predRF == testing$diagnosis
testing$GBMRight <- predGBM == testing$diagnosis
testing$LDARight <- predLDA == testing$diagnosis
testing$COMRight <- combPred == testing$diagnosis
print(nrow(testing[testing$RFRight == TRUE,]) / nrow(testing))
print(nrow(testing[testing$GBMRight == TRUE,]) / nrow(testing))
print(nrow(testing[testing$LDARight == TRUE,]) / nrow(testing))
print(nrow(testing[testing$COMRight == TRUE,]) / nrow(testing))
# Question 2
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modelRF <- train(diagnosis ~., method = "rf", data=training, trControl=trainControl(number = 4))
modelGBM <- train(diagnosis ~., method = "gbm", data=training, verbose=FALSE)
modelLDA <- train(diagnosis ~., method = "lda", data=training)
predRF <- predict(modelRF,testing)
predGBM <- predict(modelGBM,testing)
predLDA <- predict(modelLDA,testing)
predDF <- data.frame(predRF,predGBM,predLDA,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~., method="rf", data=predDF, trControl=trainControl(number = 4))
combPred <- predict(combModFit, predDF)
testing$RFRight <- predRF == testing$diagnosis
testing$GBMRight <- predGBM == testing$diagnosis
testing$LDARight <- predLDA == testing$diagnosis
testing$COMRight <- combPred == testing$diagnosis
print(nrow(testing[testing$RFRight == TRUE,]) / nrow(testing))
print(nrow(testing[testing$GBMRight == TRUE,]) / nrow(testing))
print(nrow(testing[testing$LDARight == TRUE,]) / nrow(testing))
print(nrow(testing[testing$COMRight == TRUE,]) / nrow(testing))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
View(training)
?plot.enet
??plot.enet
odelLasso <- train(CompressiveStrength ~., method="lasso", data=training)
modelLasso <- train(CompressiveStrength ~., method="lasso", data=training)
plot.enet(modelLasso)
plot.enet(modelLasso$finalModel, xvar = "penalty")
plot.enet(modelLasso$finalModel, xvar = "penalty", use.color = T)
plot.enet(modelLasso$finalModel, xvar = "Cp", use.color = T)
plot.enet(modelLasso$finalModel, xvar = "penalty", use.color = T)
plot(modelLasso$finalModel, xvar = "penalty", use.color = T)
plot(modelLasso$finalModel, xvar = "step", use.color = T)
plot(modelLasso$finalModel, xvar = "penalty", use.color = T)
library(lubridate)  # For year() function below
dat = read.csv("~/Downloads/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
install.package("forecast")
install.packages("forecast")
library(forecast)
View(training)
View(testing)
plot(decompose(training$visitsTumblr))
plot(decompose(tstrain))
plot(tstrain)
batsFit <- bats(y = tstrain)
fcast <- forecast(bastFit)
fcast <- forecast(batsFit)
plot(fcast)
batsFit <- bats(y = tstrain)
fcast <- forecast(batsFit)
tsTest = ts(testing$visitsTumblr)
accuracy(fcast,tsTest)
View(fcast)
plot(fcast); lines(tsTest,col="red"))
plot(fcast); lines(tsTest,col="red")
summary(tstrain)
tstrain
sTest = ts(testing$visitsTumblr, start=366)
ccuracy(fcast,tsTest)
accuracy(fcast,tsTest)
tsTest = ts(testing$visitsTumblr, start=366, end=(+366))
accuracy(fcast,tsTest)
plot(fcast); lines(tsTest,col="red")
plot(tsTest)
tsTest = ts(testing$visitsTumblr)
plot(tsTest)
accuracy(fcast,tsTest[1:10,1])
accuracy(fcast,tsTest)
library(lubridate)  # For year() function below
dat = read.csv("~/Downloads/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
library(quantmod)
batsFit <- bats(y = tstrain)
h <- dim(testing)[1]
fcast <- forecast(batsFit, level=95,h=h)
result <- c()
l <- lenght(fcast$lower)
for(i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c (result,a)
}
sum(result)/l*100
result <- c()
l <- length(fcast$lower)
for(i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c (result,a)
}
sum(result)/l*100
install.packages("e1071")
install.packages("e1071")
set.seed
set.seed(325)
library(e1071)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
View(training)
svmModel <- svm(CompressiveStrength ~., data = training)
preds <- predict(svmModel,testing)
library(e1071)
svmModel <- svm(CompressiveStrength ~., data = training)
preds <- predict(svmModel,testing)
testing$errorSq <- (preds - testing$CompressiveStrength)^2
sumError <- sum(testing$errorSq)
meanSumError <- sumError / nrow(testing)
RMSE <- sqrt(meanSumError)
print(RMSE)
x <- seq(from = 0, by = 0, length.out = 10)
x <- as.vector(x)
noise <- runif(n = 10, min=-0.1, max = 0.1)
nois
noise
y = x + noise
y
setwd("~/MLGitRepos/Kaggle/Prudential/")
# Step 1: Load Libraries
library(caret)
library(doParallel)
registerDoParallel(cores=2)
# Step 2 - Get and load the data
checkAndDownloadFiles <- function(){
## First check if data folder exists
if (!dir.exists("data")){
dir.create(path = "data")
}
if (!file.exists("data/train.csv")){
fileURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url = fileURL, destfile = "data/pml-training.csv", method="curl")
}
if (!file.exists("data/test.csv")){
fileURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url = fileURL, destfile = "data/pml-testing.csv", method="curl")
}
}
readInTrainingData <- function(){
filename <- "data/train.csv"
training <- read.table(filename, sep=",", skip=0, header=TRUE,
na.strings=c("NA","NaN",""," ","#DIV/0!"))
return (training)
}
checkAndDownloadFiles()
allTrainingData <- readInTrainingData()
# Step 3 - Partition training & test datasets
createTrainTest <- function(theData){
set.seed(41283)
inTrain <- createDataPartition(y = theData$Response, p=.70, list=FALSE)
training <- theData[inTrain,]
testing <- theData[-inTrain,]
return (list(training = training, testing = testing))
}
datasets <- createTrainTest(allTrainingData)
training <- datasets$training
training <- training[,colSums(is.na(training)) < 10]
any(is.na(training))
any(is.null(training))
# B. Next Remove columsn relating to Index, timestamps and windows
training <- training[,-(1:8)]
# C. Check for highly correlated features and remove unnecessary features
descrCor <- cor(training[,-ncol(training)])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
training <- training[,-highlyCorDescr]
# Step 5 - Training
lastCol <- ncol(training)
preProc.Norm <- preProcess(training[,-lastCol],method=c("center","scale"))
train.Norm <- predict(preProc.Norm, training[,-lastCol])
set.seed(360)
modelFit.Norm <- train(training$Response ~., method="rf" ,data=train.Norm,
trControl=trainControl(method="cv",number=5),
prox=TRUE,allowParallel=TRUE)
